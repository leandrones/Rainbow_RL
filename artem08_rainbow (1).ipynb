{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "artem08_rainbow (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwvpn4V_wiiE",
        "colab_type": "text"
      },
      "source": [
        "## Configurations for Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP4tlZVLw2nL",
        "colab_type": "code",
        "outputId": "20e5ab37-09af-4641-9200-35c4da419cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "!apt update"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.31)] [Waiting for headers] [Con\u001b[0m\r                                                                               \rGet:2 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connecting to cloud.r-project.o\u001b[0m\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [3 InRelease 0 B/88.7 kB 0%] [Connecting to cloud.r-pr\u001b[0m\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [3 InRelease 14.2 kB/88.7 kB 16%] [Connecting to cloud\u001b[0m\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\u001b[33m\r0% [Waiting for headers] [3 InRelease 14.2 kB/88.7 kB 16%] [Connecting to cloud\u001b[0m\u001b[33m\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 kB \u001b[0m\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:9 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [36.9 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [824 kB]\n",
            "Get:16 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [86.9 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [836 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,352 kB]\n",
            "Get:19 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,777 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [7,641 B]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [31.0 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [44.7 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,129 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [11.7 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4,247 B]\n",
            "Get:26 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [858 kB]\n",
            "Fetched 7,290 kB in 3s (2,241 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "118 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skhoMUF6wiiP",
        "colab_type": "code",
        "outputId": "3d22b281-724f-441d-a6f7-12a977ed1eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !apt install python-opengl\n",
        "    !apt install ffmpeg\n",
        "    !apt install xvfb\n",
        "    !pip install pyvirtualdisplay\n",
        "    from pyvirtualdisplay import Display\n",
        "    \n",
        "    # Start virtual display\n",
        "    dis = Display(visible=0, size=(400, 400))\n",
        "    dis.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 118 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (778 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 145113 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 118 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  xserver-common\n",
            "Recommended packages:\n",
            "  xfonts-base\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "The following packages will be upgraded:\n",
            "  xserver-common\n",
            "1 upgraded, 1 newly installed, 0 to remove and 117 not upgraded.\n",
            "Need to get 811 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xserver-common all 2:1.19.6-1ubuntu4.4 [27.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.4 [784 kB]\n",
            "Fetched 811 kB in 1s (1,029 kB/s)\n",
            "(Reading database ... 147468 files and directories currently installed.)\n",
            "Preparing to unpack .../xserver-common_2%3a1.19.6-1ubuntu4.4_all.deb ...\n",
            "Unpacking xserver-common (2:1.19.6-1ubuntu4.4) over (2:1.19.6-1ubuntu4.3) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xserver-common (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/69/ec/8221a07850d69fa3c57c02e526edd23d18c7c05d58ed103e3b19172757c1/PyVirtualDisplay-0.2.5-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/32/8f/88d636f1da22a3c573259e44cfefb46a117d3f9432e2c98b1ab4a21372ad/EasyProcess-0.2.10-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.2.10 pyvirtualdisplay-0.2.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVVJq60Nwiiq",
        "colab_type": "text"
      },
      "source": [
        "# 08. Rainbow\n",
        "\n",
        "[M. Hessel et al., \"Rainbow: Combining Improvements in Deep Reinforcement Learning.\" arXiv preprint arXiv:1710.02298, 2017.](https://arxiv.org/pdf/1710.02298.pdf)\n",
        "\n",
        "We will integrate all the following seven components into a single integrated agent, which is called Rainbow!\n",
        "\n",
        "1. DQN\n",
        "2. Double DQN\n",
        "3. Prioritized Experience Replay\n",
        "4. Dueling Network\n",
        "5. Noisy Network\n",
        "6. Categorical DQN\n",
        "7. N-step Learning\n",
        "\n",
        "This method shows an impressive performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. \n",
        "\n",
        "![rainbow](https://user-images.githubusercontent.com/14961526/60591412-61748100-9dd9-11e9-84fb-076c7a61fbab.png)\n",
        "\n",
        "However, the integration is not so simple because some of components are not independent each other, so we will look into a number of points that people especailly feel confused.\n",
        "\n",
        "1. Noisy Network <-> Dueling Network\n",
        "2. Dueling Network <-> Categorical DQN\n",
        "3. Categorical DQN <-> Double DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-3R2Fbjwii1",
        "colab_type": "code",
        "outputId": "f2793147-688c-4331-ac02-bad4c9b4a9b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "from collections import deque\n",
        "from typing import Deque, Dict, List, Tuple\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# download segment tree module\n",
        "if IN_COLAB:\n",
        "    !wget https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py\n",
        "\n",
        "from segment_tree import MinSegmentTree, SumSegmentTree"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-03 22:00:37--  https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4283 (4.2K) [text/plain]\n",
            "Saving to: ‘segment_tree.py’\n",
            "\n",
            "\rsegment_tree.py       0%[                    ]       0  --.-KB/s               \rsegment_tree.py     100%[===================>]   4.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-03 22:00:37 (119 MB/s) - ‘segment_tree.py’ saved [4283/4283]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suKVXUvtwijK",
        "colab_type": "text"
      },
      "source": [
        "## Replay buffer\n",
        "\n",
        "Same as the basic N-step buffer. \n",
        "\n",
        "(Please see *01.dqn.ipynb*, *07.n_step_learning.ipynb* for detailed description about the basic (n-step) replay buffer.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9dvQGmewijR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        obs_dim: tuple, \n",
        "        size: int, \n",
        "        batch_size: int = 32, \n",
        "        n_step: int = 1, \n",
        "        gamma: float = 0.99\n",
        "    ):\n",
        "        self.obs_buf = np.zeros([size,obs_dim[0], obs_dim[1],obs_dim[2]], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size,obs_dim[0], obs_dim[1],obs_dim[2]], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "        \n",
        "        # for N-step Learning\n",
        "        self.n_step_buffer = deque(maxlen=n_step)\n",
        "        self.n_step = n_step\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def store(\n",
        "        self, \n",
        "        obs: np.ndarray, \n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
        "        transition = (obs, act, rew, next_obs, done)\n",
        "        self.n_step_buffer.append(transition)\n",
        "\n",
        "        # single step transition is not ready\n",
        "        if len(self.n_step_buffer) < self.n_step:\n",
        "            return ()\n",
        "        \n",
        "        # make a n-step transition\n",
        "        rew, next_obs, done = self._get_n_step_info(\n",
        "            self.n_step_buffer, self.gamma\n",
        "        )\n",
        "        obs, act = self.n_step_buffer[0][:2]\n",
        "        \n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        \n",
        "        return self.n_step_buffer[0]\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "\n",
        "        return dict(\n",
        "            obs=self.obs_buf[idxs],\n",
        "            next_obs=self.next_obs_buf[idxs],\n",
        "            acts=self.acts_buf[idxs],\n",
        "            rews=self.rews_buf[idxs],\n",
        "            done=self.done_buf[idxs],\n",
        "            # for N-step Learning\n",
        "            indices=indices,\n",
        "        )\n",
        "    \n",
        "    def sample_batch_from_idxs(\n",
        "        self, idxs: np.ndarray\n",
        "    ) -> Dict[str, np.ndarray]:\n",
        "        # for N-step Learning\n",
        "        return dict(\n",
        "            obs=self.obs_buf[idxs],\n",
        "            next_obs=self.next_obs_buf[idxs],\n",
        "            acts=self.acts_buf[idxs],\n",
        "            rews=self.rews_buf[idxs],\n",
        "            done=self.done_buf[idxs],\n",
        "        )\n",
        "    \n",
        "    def _get_n_step_info(\n",
        "        self, n_step_buffer: Deque, gamma: float\n",
        "    ) -> Tuple[np.int64, np.ndarray, bool]:\n",
        "        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n",
        "        # info of the last transition\n",
        "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
        "\n",
        "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
        "            r, n_o, d = transition[-3:]\n",
        "\n",
        "            rew = r + gamma * rew * (1 - d)\n",
        "            next_obs, done = (n_o, d) if d else (next_obs, done)\n",
        "\n",
        "        return rew, next_obs, done\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne7mTRHMwijw",
        "colab_type": "text"
      },
      "source": [
        "## Prioritized replay Buffer\n",
        "\n",
        "`store` method returns boolean in order to inform if a N-step transition has been generated.\n",
        "\n",
        "(Please see *02.per.ipynb* for detailed description about PER.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLWJWJ5xwikD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrioritizedReplayBuffer(ReplayBuffer):\n",
        "    \"\"\"Prioritized Replay buffer.\n",
        "    \n",
        "    Attributes:\n",
        "        max_priority (float): max priority\n",
        "        tree_ptr (int): next index of tree\n",
        "        alpha (float): alpha parameter for prioritized replay buffer\n",
        "        sum_tree (SumSegmentTree): sum tree for prior\n",
        "        min_tree (MinSegmentTree): min tree for min prior to get max weight\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        obs_dim: tuple, \n",
        "        size: int, \n",
        "        batch_size: int = 32, \n",
        "        alpha: float = 0.6,\n",
        "        n_step: int = 1, \n",
        "        gamma: float = 0.99,\n",
        "    ):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        assert alpha >= 0\n",
        "        \n",
        "        super(PrioritizedReplayBuffer, self).__init__(\n",
        "            obs_dim, size, batch_size, n_step, gamma\n",
        "        )\n",
        "        self.max_priority, self.tree_ptr = 1.0, 0\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        # capacity must be positive and a power of 2.\n",
        "        tree_capacity = 1\n",
        "        while tree_capacity < self.max_size:\n",
        "            tree_capacity *= 2\n",
        "\n",
        "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
        "        self.min_tree = MinSegmentTree(tree_capacity)\n",
        "        \n",
        "    def store(\n",
        "        self, \n",
        "        obs: np.ndarray, \n",
        "        act: int, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
        "        \"\"\"Store experience and priority.\"\"\"\n",
        "        transition = super().store(obs, act, rew, next_obs, done)\n",
        "        \n",
        "        if transition:\n",
        "            self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
        "            self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
        "            self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
        "        \n",
        "        return transition\n",
        "\n",
        "    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Sample a batch of experiences.\"\"\"\n",
        "        assert len(self) >= self.batch_size\n",
        "        assert beta > 0\n",
        "        \n",
        "        indices = self._sample_proportional()\n",
        "        \n",
        "        obs = self.obs_buf[indices]\n",
        "        next_obs = self.next_obs_buf[indices]\n",
        "        acts = self.acts_buf[indices]\n",
        "        rews = self.rews_buf[indices]\n",
        "        done = self.done_buf[indices]\n",
        "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
        "        \n",
        "        return dict(\n",
        "            obs=obs,\n",
        "            next_obs=next_obs,\n",
        "            acts=acts,\n",
        "            rews=rews,\n",
        "            done=done,\n",
        "            weights=weights,\n",
        "            indices=indices,\n",
        "        )\n",
        "        \n",
        "    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n",
        "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
        "        assert len(indices) == len(priorities)\n",
        "\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            assert priority > 0\n",
        "            assert 0 <= idx < len(self)\n",
        "\n",
        "            self.sum_tree[idx] = priority ** self.alpha\n",
        "            self.min_tree[idx] = priority ** self.alpha\n",
        "\n",
        "            self.max_priority = max(self.max_priority, priority)\n",
        "            \n",
        "    def _sample_proportional(self) -> List[int]:\n",
        "        \"\"\"Sample indices based on proportions.\"\"\"\n",
        "        indices = []\n",
        "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
        "        segment = p_total / self.batch_size\n",
        "        \n",
        "        for i in range(self.batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            upperbound = random.uniform(a, b)\n",
        "            idx = self.sum_tree.retrieve(upperbound)\n",
        "            indices.append(idx)\n",
        "            \n",
        "        return indices\n",
        "    \n",
        "    def _calculate_weight(self, idx: int, beta: float):\n",
        "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
        "        # get max weight\n",
        "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
        "        max_weight = (p_min * len(self)) ** (-beta)\n",
        "        \n",
        "        # calculate weights\n",
        "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
        "        weight = (p_sample * len(self)) ** (-beta)\n",
        "        weight = weight / max_weight\n",
        "        \n",
        "        return weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tj9vWX0wik8",
        "colab_type": "text"
      },
      "source": [
        "## Noisy Layer\n",
        "\n",
        "Please see *05.noisy_net.ipynb* for detailed description.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- https://github.com/higgsfield/RL-Adventure/blob/master/5.noisy%20dqn.ipynb\n",
        "- https://github.com/Kaixhin/Rainbow/blob/master/model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ2llmlMwilC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoisyLinear(nn.Module):\n",
        "    \"\"\"Noisy linear module for NoisyNet.\n",
        "    \n",
        "    \n",
        "        \n",
        "    Attributes:\n",
        "        in_features (int): input size of linear module\n",
        "        out_features (int): output size of linear module\n",
        "        std_init (float): initial std value\n",
        "        weight_mu (nn.Parameter): mean value weight parameter\n",
        "        weight_sigma (nn.Parameter): std value weight parameter\n",
        "        bias_mu (nn.Parameter): mean value bias parameter\n",
        "        bias_sigma (nn.Parameter): std value bias parameter\n",
        "        \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_features: int, \n",
        "        out_features: int, \n",
        "        std_init: float = 0.5,\n",
        "    ):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(NoisyLinear, self).__init__()\n",
        "        \n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.std_init = std_init\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features)\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"weight_epsilon\", torch.Tensor(out_features, in_features)\n",
        "        )\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "        self.reset_noise()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
        "        mu_range = 1 / math.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(\n",
        "            self.std_init / math.sqrt(self.in_features)\n",
        "        )\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(\n",
        "            self.std_init / math.sqrt(self.out_features)\n",
        "        )\n",
        "\n",
        "    def reset_noise(self):\n",
        "        \"\"\"Make new noise.\"\"\"\n",
        "        epsilon_in = self.scale_noise(self.in_features)\n",
        "        epsilon_out = self.scale_noise(self.out_features)\n",
        "\n",
        "        # outer product\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(epsilon_out)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\n",
        "        \n",
        "        We don't use separate statements on train / eval mode.\n",
        "        It doesn't show remarkable difference of performance.\n",
        "        \"\"\"\n",
        "        return F.linear(\n",
        "            x,\n",
        "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
        "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
        "        )\n",
        "    \n",
        "    @staticmethod\n",
        "    def scale_noise(size: int) -> torch.Tensor:\n",
        "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
        "        x = torch.FloatTensor(np.random.normal(loc=0.0, scale=1.0, size=size))\n",
        "\n",
        "        return x.sign().mul(x.abs().sqrt())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7_8PKHJwill",
        "colab_type": "text"
      },
      "source": [
        "## NoisyNet + DuelingNet + Categorical DQN\n",
        "\n",
        "#### NoisyNet + DuelingNet\n",
        "\n",
        "NoisyLinear is employed for the last two layers of advantage and value layers. The noise should be reset at evey update step.\n",
        "\n",
        "#### DuelingNet + Categorical DQN\n",
        "\n",
        "The dueling network architecture is adapted for use with return distributions. The network has a shared representation, which is then fed into a value stream with atom_size outputs, and into an advantage stream with atom_size × out_dim outputs. For each atom, the value and advantage streams are aggregated, as in dueling DQN, and then passed through a softmax layer to obtain the normalized parametric distributions used to estimate the returns’ distributions.\n",
        "\n",
        "```\n",
        "        advantage = self.advantage_layer(adv_hid).view(-1, self.out_dim, self.atom_size)\n",
        "        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n",
        "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
        "        \n",
        "        dist = F.softmax(q_atoms, dim=-1)\n",
        "```\n",
        "\n",
        "(Please see *04.dueling.ipynb*, *05.noisy_net.ipynb*, *06.categorical_dqn.ipynb* for detailed description of each component's network architecture.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x5lHQbMwilx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_dim: tuple, \n",
        "        out_dim: int, \n",
        "        atom_size: int, \n",
        "        support: torch.Tensor\n",
        "    ):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(Network, self).__init__()\n",
        "        \n",
        "        self.support = support\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.atom_size = atom_size\n",
        "\n",
        "        # set common feature layer\n",
        "        self.feature_layer = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.flatten = Flatten()\n",
        "        \n",
        "        # set advantage layer\n",
        "        self.advantage_hidden_layer = NoisyLinear(self._feature_size(), 128)\n",
        "        self.advantage_layer = NoisyLinear(128, out_dim * atom_size)\n",
        "\n",
        "        # set value layer\n",
        "        self.value_hidden_layer = NoisyLinear(self._feature_size(), 128)\n",
        "        self.value_layer = NoisyLinear(128, atom_size)\n",
        "\n",
        "    def _feature_size(self):\n",
        "        return self.feature_layer(torch.zeros(1, *self.in_dim)).view(1, -1).size(1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        dist = self.dist(x)\n",
        "        q = torch.sum(dist * self.support, dim=2)\n",
        "        \n",
        "        return q\n",
        "    \n",
        "    def dist(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get distribution for atoms.\"\"\"\n",
        "        feature = self.feature_layer(x)\n",
        "        fl = self.flatten(feature)\n",
        "        adv_hid = F.relu(self.advantage_hidden_layer(fl))\n",
        "        val_hid = F.relu(self.value_hidden_layer(fl))\n",
        "        \n",
        "        advantage = self.advantage_layer(adv_hid).view(\n",
        "            -1, self.out_dim, self.atom_size\n",
        "        )\n",
        "        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n",
        "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
        "        \n",
        "        dist = F.softmax(q_atoms, dim=-1)\n",
        "        dist = dist.clamp(min=1e-3)  # for avoiding nans\n",
        "        \n",
        "        return dist\n",
        "    \n",
        "    def reset_noise(self):\n",
        "        \"\"\"Reset all noisy layers.\"\"\"\n",
        "        self.advantage_hidden_layer.reset_noise()\n",
        "        self.advantage_layer.reset_noise()\n",
        "        self.value_hidden_layer.reset_noise()\n",
        "        self.value_layer.reset_noise()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QwIEYoEwimM",
        "colab_type": "text"
      },
      "source": [
        "## Rainbow Agent\n",
        "\n",
        "Here is a summary of DQNAgent class.\n",
        "\n",
        "| Method           | Note                                                 |\n",
        "| ---              | ---                                                  |\n",
        "|select_action     | select an action from the input state.               |\n",
        "|step              | take an action and return the response of the env.   |\n",
        "|compute_dqn_loss  | return dqn loss.                                     |\n",
        "|update_model      | update the model by gradient descent.                |\n",
        "|target_hard_update| hard update from the local model to the target model.|\n",
        "|train             | train the agent during num_frames.                   |\n",
        "|test              | test the agent (1 episode).                          |\n",
        "|plot              | plot the training progresses.                        |\n",
        "\n",
        "#### Categorical DQN + Double DQN\n",
        "\n",
        "The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Here, we use `self.dqn` instead of `self.dqn_target` to obtain the target actions.\n",
        "\n",
        "```\n",
        "        # Categorical DQN + Double DQN\n",
        "        # target_dqn is used when we don't employ double DQN\n",
        "        next_action = self.dqn(next_state).argmax(1)\n",
        "        next_dist = self.dqn_target.dist(next_state)\n",
        "        next_dist = next_dist[range(self.batch_size), next_action]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddjVPSf8wimU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"DQN Agent interacting with environment.\n",
        "    \n",
        "    Attribute:\n",
        "        env (gym.Env): openAI Gym environment\n",
        "        memory (PrioritizedReplayBuffer): replay memory to store transitions\n",
        "        batch_size (int): batch size for sampling\n",
        "        target_update (int): period for target model's hard update\n",
        "        gamma (float): discount factor\n",
        "        dqn (Network): model to train and select actions\n",
        "        dqn_target (Network): target model to update\n",
        "        optimizer (torch.optim): optimizer for training dqn\n",
        "        transition (list): transition information including \n",
        "                           state, action, reward, next_state, done\n",
        "        v_min (float): min value of support\n",
        "        v_max (float): max value of support\n",
        "        atom_size (int): the unit number of support\n",
        "        support (torch.Tensor): support for categorical dqn\n",
        "        use_n_step (bool): whether to use n_step memory\n",
        "        n_step (int): step number to calculate n-step td error\n",
        "        memory_n (ReplayBuffer): n-step replay buffer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        env: gym.Env,\n",
        "        memory_size: int,\n",
        "        batch_size: int,\n",
        "        target_update: int,\n",
        "        gamma: float = 0.99,\n",
        "        # PER parameters\n",
        "        alpha: float = 0.2,\n",
        "        beta: float = 0.6,\n",
        "        prior_eps: float = 1e-6,\n",
        "        # Categorical DQN parameters\n",
        "        v_min: float = -10.0,\n",
        "        v_max: float = 10.0,\n",
        "        atom_size: int = 51,\n",
        "        # N-step Learning\n",
        "        n_step: int = 3,\n",
        "    ):\n",
        "        \"\"\"Initialization.\n",
        "        \n",
        "        Args:\n",
        "            env (gym.Env): openAI Gym environment\n",
        "            memory_size (int): length of memory\n",
        "            batch_size (int): batch size for sampling\n",
        "            target_update (int): period for target model's hard update\n",
        "            lr (float): learning rate\n",
        "            gamma (float): discount factor\n",
        "            alpha (float): determines how much prioritization is used\n",
        "            beta (float): determines how much importance sampling is used\n",
        "            prior_eps (float): guarantees every transition can be sampled\n",
        "            v_min (float): min value of support\n",
        "            v_max (float): max value of support\n",
        "            atom_size (int): the unit number of support\n",
        "            n_step (int): step number to calculate n-step td error\n",
        "        \"\"\"\n",
        "        obs_dim = env.observation_space.shape\n",
        "        action_dim = env.action_space.n\n",
        "        \n",
        "        self.env = env\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update = target_update\n",
        "        self.gamma = gamma\n",
        "        # NoisyNet: All attributes related to epsilon are removed\n",
        "        \n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        print(self.device)\n",
        "        \n",
        "        # PER\n",
        "        # memory for 1-step Learning\n",
        "        self.beta = beta\n",
        "        self.prior_eps = prior_eps\n",
        "        self.memory = PrioritizedReplayBuffer(\n",
        "            obs_dim, memory_size, batch_size, alpha=alpha\n",
        "        )\n",
        "        \n",
        "        # memory for N-step Learning\n",
        "        self.use_n_step = True if n_step > 1 else False\n",
        "        if self.use_n_step:\n",
        "            self.n_step = n_step\n",
        "            self.memory_n = ReplayBuffer(\n",
        "                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma\n",
        "            )\n",
        "            \n",
        "        # Categorical DQN parameters\n",
        "        self.v_min = v_min\n",
        "        self.v_max = v_max\n",
        "        self.atom_size = atom_size\n",
        "        self.support = torch.linspace(\n",
        "            self.v_min, self.v_max, self.atom_size\n",
        "        ).to(self.device)\n",
        "\n",
        "        # networks: dqn, dqn_target\n",
        "        self.dqn = Network(\n",
        "            obs_dim, action_dim, self.atom_size, self.support\n",
        "        ).to(self.device)\n",
        "        self.dqn_target = Network(\n",
        "            obs_dim, action_dim, self.atom_size, self.support\n",
        "        ).to(self.device)\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "        self.dqn_target.eval()\n",
        "        \n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
        "\n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "        #scores history\n",
        "        self.scores = []\n",
        "        # mode: train / test\n",
        "        self.is_test = False\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        # NoisyNet: no epsilon greedy action selection\n",
        "        state = np.concatenate(state._frames, axis=0)\n",
        "        state_exp_dims = np.expand_dims(state,axis=0)\n",
        "        # print(state_exp_dims.shape)\n",
        "        selected_action = self.dqn(\n",
        "            torch.FloatTensor(state_exp_dims).to(self.device)\n",
        "        ).argmax()\n",
        "        selected_action = selected_action.detach().cpu().numpy()\n",
        "        \n",
        "        if not self.is_test:\n",
        "            self.transition = [state, selected_action]\n",
        "        \n",
        "        return selected_action\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        #print(action)\n",
        "\n",
        "        # sh =  next_state.shape\n",
        "        # #next_state = np.reshape( next_state, [1,sh[0],sh[1],sh[2]])\n",
        "        # next_state = np.transpose(next_state, (2,0,1))\n",
        "  \n",
        "        if not self.is_test:\n",
        "            self.transition += [reward, next_state, done]\n",
        "            \n",
        "            # N-step transition\n",
        "            if self.use_n_step:\n",
        "                one_step_transition = self.memory_n.store(*self.transition)\n",
        "            # 1-step transition\n",
        "            else:\n",
        "                one_step_transition = self.transition\n",
        "\n",
        "            # add a single step transition\n",
        "            if one_step_transition:\n",
        "                self.memory.store(*one_step_transition)\n",
        "    \n",
        "        return next_state, reward, done\n",
        "\n",
        "    def update_model(self) -> torch.Tensor:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        # PER needs beta to calculate weights\n",
        "        samples = self.memory.sample_batch(self.beta)\n",
        "        weights = torch.FloatTensor(\n",
        "            samples[\"weights\"].reshape(-1, 1)\n",
        "        ).to(self.device)\n",
        "        indices = samples[\"indices\"]\n",
        "        \n",
        "        # 1-step Learning loss\n",
        "        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n",
        "        \n",
        "        # PER: importance sampling before average\n",
        "        loss = torch.mean(elementwise_loss * weights)\n",
        "        \n",
        "        # N-step Learning loss\n",
        "        # we are gonna combine 1-step loss and n-step loss so as to\n",
        "        # prevent high-variance. The original rainbow employs n-step loss only.\n",
        "        if self.use_n_step:\n",
        "            gamma = self.gamma ** self.n_step\n",
        "            samples = self.memory_n.sample_batch_from_idxs(indices)\n",
        "            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n",
        "            elementwise_loss += elementwise_loss_n_loss\n",
        "            \n",
        "            # PER: importance sampling before average\n",
        "            loss = torch.mean(elementwise_loss * weights)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(self.dqn.parameters(), 10.0)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # PER: update priorities\n",
        "        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
        "        new_priorities = loss_for_prior + self.prior_eps\n",
        "        self.memory.update_priorities(indices, new_priorities)\n",
        "        \n",
        "        # NoisyNet: reset noise\n",
        "        self.dqn.reset_noise()\n",
        "        self.dqn_target.reset_noise()\n",
        "\n",
        "        return loss.item()\n",
        "        \n",
        "    def train(self, num_frames: int, plotting_interval: int = 2000):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        self.is_test = False\n",
        "        \n",
        "        state = self.env.reset()\n",
        "\n",
        "        \n",
        "        # sh =  state.shape\n",
        "        # #state = np.reshape( state, [1,sh[0],sh[1],sh[2]])\n",
        "        \n",
        "        # state = np.transpose(state, (2,0,1))\n",
        "\n",
        "        \n",
        "        update_cnt = 0\n",
        "        losses = []\n",
        "        self.scores = []\n",
        "        strategies = []\n",
        "        score = 0\n",
        "\n",
        "        for frame_idx in range(1, num_frames + 1):\n",
        "            action = self.select_action(state)\n",
        "            strategies.append(action)\n",
        "            next_state, reward, done = self.step(action)\n",
        "            \n",
        "            # print('rew = ',reward)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            \n",
        "            # NoisyNet: removed decrease of epsilon\n",
        "            \n",
        "            # PER: increase beta\n",
        "            fraction = min(frame_idx / num_frames, 1.0)\n",
        "            self.beta = self.beta + fraction * (1.0 - self.beta)\n",
        "\n",
        "            # if episode ends\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                # sh =  state.shape\n",
        "                # #state = np.reshape( state, [1,sh[0],sh[1],sh[2]])\n",
        "                # state = np.transpose(state, (2,0,1))\n",
        "                \n",
        "                self.scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "            # if training is ready\n",
        "            if len(self.memory) >= self.batch_size:\n",
        "                loss = self.update_model()\n",
        "                losses.append(loss)\n",
        "                update_cnt += 1\n",
        "                \n",
        "                # if hard update is needed\n",
        "                if update_cnt % self.target_update == 0:\n",
        "                    self._target_hard_update()\n",
        "\n",
        "            # plotting\n",
        "            if frame_idx % plotting_interval == 0:\n",
        "                self._plot(frame_idx, self.scores, losses, strategies)\n",
        "                \n",
        "        self.env.close()\n",
        "                \n",
        "    def test(self) -> None:\n",
        "        \"\"\"Test the agent.\"\"\"\n",
        "        self.is_test = True\n",
        "        \n",
        "        state = self.env.reset()\n",
        "        # sh =  state.shape\n",
        "        # state = np.reshape( state, [1,sh[0],sh[1],sh[2]])\n",
        "        # state = np.transpose(state, (0,3,1,2))\n",
        "        done = False\n",
        "        score = 0\n",
        "        \n",
        "        while not done:\n",
        "            self.env.render()\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "        \n",
        "        print(\"score: \", score)\n",
        "        self.env.close()\n",
        "\n",
        "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray], gamma: float) -> torch.Tensor:\n",
        "        \"\"\"Return categorical dqn loss.\"\"\"\n",
        "        device = self.device  # for shortening the following lines\n",
        "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "        action = torch.LongTensor(samples[\"acts\"]).to(device)\n",
        "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "        \n",
        "        # Categorical DQN algorithm\n",
        "        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Double DQN\n",
        "            next_action = self.dqn(next_state).argmax(1)\n",
        "            next_dist = self.dqn_target.dist(next_state)\n",
        "            next_dist = next_dist[range(self.batch_size), next_action]\n",
        "\n",
        "            t_z = reward + (1 - done) * gamma * self.support\n",
        "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
        "            b = (t_z - self.v_min) / delta_z\n",
        "            l = b.floor().long()\n",
        "            u = b.ceil().long()\n",
        "\n",
        "            offset = (\n",
        "                torch.linspace(\n",
        "                    0, (self.batch_size - 1) * self.atom_size, self.batch_size\n",
        "                ).long()\n",
        "                .unsqueeze(1)\n",
        "                .expand(self.batch_size, self.atom_size)\n",
        "                .to(self.device)\n",
        "            )\n",
        "\n",
        "            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n",
        "            proj_dist.view(-1).index_add_(\n",
        "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
        "            )\n",
        "            proj_dist.view(-1).index_add_(\n",
        "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
        "            )\n",
        "\n",
        "        dist = self.dqn.dist(state)\n",
        "        log_p = torch.log(dist[range(self.batch_size), action])\n",
        "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
        "\n",
        "        return elementwise_loss\n",
        "\n",
        "    def _target_hard_update(self):\n",
        "        \"\"\"Hard update: target <- local.\"\"\"\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "                \n",
        "    def _plot(\n",
        "        self, \n",
        "        frame_idx: int, \n",
        "        scores: List[float], \n",
        "        losses: List[float],\n",
        "        strategies: List[float]\n",
        "    ):\n",
        "        \"\"\"Plot the training progresses.\"\"\"\n",
        "        print('scor = ',scores[-1])\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        plt.subplot(131)\n",
        "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
        "        plt.plot(scores)\n",
        "        plt.subplot(132)\n",
        "        plt.title('loss')\n",
        "        plt.plot(losses)\n",
        "        plt.subplot(133)\n",
        "        plt.title('strategy')\n",
        "        plt.plot(strategies)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXVNYmjnwimx",
        "colab_type": "text"
      },
      "source": [
        "## Environment\n",
        "\n",
        "You can see the [code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) and [configurations](https://github.com/openai/gym/blob/master/gym/envs/__init__.py#L53) of CartPole-v0 from OpenAI's repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCYlsuGQz_2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Source: https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import gym\n",
        "from gym import spaces\n",
        "import cv2\n",
        "\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        super(NoopResetEnv, self).__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset()\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = np.random.randint(1, self.noop_max + 1)\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(0)\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        super(EpisodicLifeEnv, self).__init__(env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "        self.was_real_reset = False\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset()\n",
        "            self.was_real_reset = True\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "            self.was_real_reset = False\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1))\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class ClippedRewardsWrapper(gym.RewardWrapper):\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Change all the positive rewards to 1, negative to -1 and keep zero.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not belive how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0]*k, shp[1], shp[2]))\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ChannelsFirstImageShape(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Change image shape to CWH\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ChannelsFirstImageShape, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.swapaxes(observation, 2, 0)\n",
        "\n",
        "\n",
        "class MainGymWrapper():\n",
        "\n",
        "    @staticmethod\n",
        "    def wrap(env):\n",
        "        env = NoopResetEnv(env, noop_max=30)\n",
        "        if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "            env = FireResetEnv(env)\n",
        "        env = ProcessFrame84(env)\n",
        "        env = ChannelsFirstImageShape(env)\n",
        "        env = FrameStack(env, 4)\n",
        "        # env = ClippedRewardsWrapper(env)\n",
        "        return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR86KwI6winF",
        "colab_type": "code",
        "outputId": "9f81f410-b80e-4e85-8c9d-ea42973b48fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# environment\n",
        "env_id = \"Qbert-v4\"\n",
        "env = MainGymWrapper.wrap(gym.make(env_id))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lciqt6El6NyJ",
        "colab_type": "code",
        "outputId": "709f0e5e-2f02-426c-f283-ee0eeeaa6990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env.observation_space.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 84, 84)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foVF--jEwinm",
        "colab_type": "text"
      },
      "source": [
        "## Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we14BzEuwinr",
        "colab_type": "code",
        "outputId": "befc8de4-bba2-45aa-a61c-2c2791820070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 777\n",
        "\n",
        "def seed_torch(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.backends.cudnn.enabled:\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "seed_torch(seed)\n",
        "env.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[777, 398687705]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj8qTFGSwioH",
        "colab_type": "text"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZqTdY-6wioO",
        "colab_type": "code",
        "outputId": "0c1bb0ef-7de8-497a-cb07-c072c51d3850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# parameters\n",
        "num_frames = 10000\n",
        "memory_size = 10000\n",
        "batch_size = 32\n",
        "target_update = 1000\n",
        "gamma = 0.9\n",
        "alpha = 0.5\n",
        "\n",
        "# train\n",
        "agent = DQNAgent(env, memory_size, batch_size, target_update, gamma = gamma, alpha = alpha)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ2zUUIiwion",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoUQETb7wios",
        "colab_type": "code",
        "outputId": "b8231605-ffd0-4270-9f18-aab66f2e8a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "agent.train(num_frames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIUAAAE/CAYAAADYJXMZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUj3PfNMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x360 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUGjzTunbFnP",
        "colab_type": "code",
        "outputId": "5bf30c69-437d-4214-f3a0-4ebbd5e5cdaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python rainbow.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Namespace(no_dist=False, no_double=False, no_duel=False, no_lookahead=False, no_noise=False, no_per=False)\n",
            "WARNING:tensorflow:From rainbow.py:252: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-03-03 22:03:23.574880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-03-03 22:03:23.621282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-03 22:03:23.622098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-03 22:03:23.633931: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-03 22:03:23.903238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-03 22:03:24.055883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-03 22:03:24.084002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-03 22:03:24.358518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-03 22:03:24.390572: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-03 22:03:24.922163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-03 22:03:24.922446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-03 22:03:24.923452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-03 22:03:24.924314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-03 22:03:24.957542: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2020-03-03 22:03:24.960818: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d6f2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-03-03 22:03:24.960860: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-03-03 22:03:25.054540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-03 22:03:25.055462: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d6f480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-03-03 22:03:25.055503: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-03-03 22:03:25.056839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-03 22:03:25.057532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-03 22:03:25.057600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-03 22:03:25.057634: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-03 22:03:25.057665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-03 22:03:25.057694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-03 22:03:25.057721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-03 22:03:25.057748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-03 22:03:25.057775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-03 22:03:25.057881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-03 22:03:25.058619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-03 22:03:25.059290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-03 22:03:25.065055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-03 22:03:25.066836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-03 22:03:25.066883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-03 22:03:25.066908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-03 22:03:25.069420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-03 22:03:25.070195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-03 22:03:25.070894: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-03-03 22:03:25.070942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From rainbow.py:199: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:266: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From rainbow.py:455: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:463: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:277: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:277: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:289: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:298: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From rainbow.py:303: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:305: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:309: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:310: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:312: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From rainbow.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "2020-03-03 22:06:19.121038: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-03 22:06:20.467195: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "Updating target\n",
            "Summary of last 10 episodes\n",
            "Max reward: 12.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.448454\n",
            "T: 80001\n",
            "T/sec: 462\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.400000\n",
            "T: 80799\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.500000\n",
            "T: 81757\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.000000\n",
            "T: 82658\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.500000\n",
            "T: 83544\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.000000\n",
            "T: 84419\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.600000\n",
            "T: 85319\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.100000\n",
            "T: 86180\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.700000\n",
            "T: 86999\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.000000\n",
            "T: 87847\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 4.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 0.500000\n",
            "T: 88590\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 3.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 0.600000\n",
            "T: 89314\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 7.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 0.700000\n",
            "T: 90044\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 0.600000\n",
            "T: 90790\n",
            "T/sec: 60\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 91685\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 3.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 0.500000\n",
            "T: 92464\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 8.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.600000\n",
            "T: 93316\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 7.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.400000\n",
            "T: 94076\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 7.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 94796\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 95586\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 96502\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.600000\n",
            "T: 97242\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 98155\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.500000\n",
            "T: 98966\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.700000\n",
            "T: 99864\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 100624\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 101412\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 102183\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 103037\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 103838\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 7.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.400000\n",
            "T: 104869\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 0.600000\n",
            "T: 105658\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 106475\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.300000\n",
            "T: 107235\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.600000\n",
            "T: 108004\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.700000\n",
            "T: 108767\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 7.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.400000\n",
            "T: 109828\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.300000\n",
            "T: 110692\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.600000\n",
            "T: 111603\n",
            "T/sec: 58\n",
            "Updating target\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.400000\n",
            "T: 112434\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 113219\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.500000\n",
            "T: 114043\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 114954\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.700000\n",
            "T: 115766\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 116557\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.100000\n",
            "T: 117287\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 118117\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 118950\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.900000\n",
            "T: 119761\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 120601\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.600000\n",
            "T: 121389\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.300000\n",
            "T: 122164\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 122932\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 123711\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 124500\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 125300\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 126096\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 126859\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 127689\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.600000\n",
            "T: 128431\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.100000\n",
            "T: 129280\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.500000\n",
            "T: 130159\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 131055\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.300000\n",
            "T: 131916\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 132822\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 133609\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.100000\n",
            "T: 134437\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 135269\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 136082\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 136879\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 137795\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.600000\n",
            "T: 138710\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 139470\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.600000\n",
            "T: 140326\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.300000\n",
            "T: 141060\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.600000\n",
            "T: 141848\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 142731\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.400000\n",
            "T: 143504\n",
            "T/sec: 59\n",
            "Updating target\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.900000\n",
            "T: 144329\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.600000\n",
            "T: 145238\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 146034\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 146978\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 147839\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 148626\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.900000\n",
            "T: 149574\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.400000\n",
            "T: 150560\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.900000\n",
            "T: 151478\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.300000\n",
            "T: 152367\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 153179\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 154013\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 154855\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 155620\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.900000\n",
            "T: 156473\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.500000\n",
            "T: 157276\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.700000\n",
            "T: 158062\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.300000\n",
            "T: 158939\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 1.000000\n",
            "Avg reward: 2.700000\n",
            "T: 159768\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 160608\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.900000\n",
            "T: 161385\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 162145\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 163062\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 163897\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 164734\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 165510\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 166343\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.700000\n",
            "T: 167188\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 167985\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.200000\n",
            "T: 168750\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 169577\n",
            "T/sec: 56\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.900000\n",
            "T: 170382\n",
            "T/sec: 56\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.900000\n",
            "T: 171296\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.400000\n",
            "T: 172299\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 173090\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 173961\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 174862\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.400000\n",
            "T: 175701\n",
            "T/sec: 59\n",
            "Updating target\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.700000\n",
            "T: 176470\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.700000\n",
            "T: 177382\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 178116\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.100000\n",
            "T: 178855\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 179773\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.800000\n",
            "T: 180643\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.900000\n",
            "T: 181401\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.700000\n",
            "T: 182135\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.600000\n",
            "T: 183057\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 183810\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 184583\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 185283\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 186087\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 186781\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 187538\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.100000\n",
            "T: 188373\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 189082\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 189901\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 7.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 190639\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 7.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 191517\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.800000\n",
            "T: 192253\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 192983\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 193762\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 8.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 194603\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.900000\n",
            "T: 195293\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.800000\n",
            "T: 196139\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 196946\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 7.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.600000\n",
            "T: 197758\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.600000\n",
            "T: 198576\n",
            "T/sec: 56\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 199584\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.600000\n",
            "T: 200386\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 201133\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.800000\n",
            "T: 201916\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.900000\n",
            "T: 202780\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.800000\n",
            "T: 203646\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.800000\n",
            "T: 204583\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 1.000000\n",
            "Avg reward: 3.500000\n",
            "T: 205381\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 206243\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 207190\n",
            "T/sec: 58\n",
            "Updating target\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 208096\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.900000\n",
            "T: 208837\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 209569\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.900000\n",
            "T: 210539\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 1.000000\n",
            "Avg reward: 3.100000\n",
            "T: 211436\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 212261\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 7.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.100000\n",
            "T: 213163\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 213943\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 1.000000\n",
            "Avg reward: 3.300000\n",
            "T: 214748\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.700000\n",
            "T: 215640\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 1.800000\n",
            "T: 216493\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.100000\n",
            "T: 217364\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 218138\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 219008\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 219888\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.200000\n",
            "T: 220731\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.100000\n",
            "T: 221589\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.400000\n",
            "T: 222413\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 223253\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.200000\n",
            "T: 224067\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 224901\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.400000\n",
            "T: 225714\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 226592\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.400000\n",
            "T: 227463\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 228338\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 229212\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 230162\n",
            "T/sec: 56\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.800000\n",
            "T: 231031\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 231915\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 232782\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 233714\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 8.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 234671\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 235553\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 236414\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 237280\n",
            "T/sec: 56\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 238148\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.400000\n",
            "T: 238968\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 239838\n",
            "T/sec: 58\n",
            "Updating target\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 240659\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 241517\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 242366\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 243227\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 244102\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 244966\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 245903\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 246774\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 247651\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 248513\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 249390\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 250239\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.600000\n",
            "T: 251020\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 251848\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 252727\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 253602\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 254429\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 255294\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 256175\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 257046\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 257925\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 258796\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 1.000000\n",
            "Avg reward: 3.500000\n",
            "T: 259707\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 260545\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.600000\n",
            "T: 261390\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.400000\n",
            "T: 262256\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.900000\n",
            "T: 263133\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 264004\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 264829\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.200000\n",
            "T: 265692\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 266593\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 267427\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.400000\n",
            "T: 268269\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 269151\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.400000\n",
            "T: 270029\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 270889\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.800000\n",
            "T: 271701\n",
            "T/sec: 58\n",
            "Updating target\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 272539\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.400000\n",
            "T: 273410\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.200000\n",
            "T: 274357\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.600000\n",
            "T: 275164\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 275993\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.100000\n",
            "T: 276900\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 277770\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.000000\n",
            "T: 278587\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.400000\n",
            "T: 279410\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 280261\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 281130\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 281944\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 282753\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 283564\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 284387\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.400000\n",
            "T: 285298\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.400000\n",
            "T: 286162\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.200000\n",
            "T: 287038\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 287835\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.600000\n",
            "T: 288697\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 289469\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 290313\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.800000\n",
            "T: 291193\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 292044\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 292869\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 1.000000\n",
            "Avg reward: 3.300000\n",
            "T: 293684\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 294498\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 1.000000\n",
            "Avg reward: 3.400000\n",
            "T: 295247\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 296061\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 7.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 296864\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 297684\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 1.000000\n",
            "Avg reward: 3.200000\n",
            "T: 298430\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.400000\n",
            "T: 299342\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 300205\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.100000\n",
            "T: 301034\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.200000\n",
            "T: 301953\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 302784\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 303617\n",
            "T/sec: 58\n",
            "Updating target\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 304433\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.100000\n",
            "T: 305226\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 306038\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.800000\n",
            "T: 306869\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.400000\n",
            "T: 307708\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.900000\n",
            "T: 308492\n",
            "T/sec: 57\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.400000\n",
            "T: 309364\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 310219\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 311068\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 311961\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 312775\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.400000\n",
            "T: 313574\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 1.000000\n",
            "Avg reward: 3.800000\n",
            "T: 314498\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 315370\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 316263\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 317065\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 317963\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.500000\n",
            "T: 318766\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.700000\n",
            "T: 319675\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.900000\n",
            "T: 320626\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.400000\n",
            "T: 321444\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 322214\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.900000\n",
            "T: 323178\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.800000\n",
            "T: 324051\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.800000\n",
            "T: 324972\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 6.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.400000\n",
            "T: 325751\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.500000\n",
            "T: 326612\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.800000\n",
            "T: 327375\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 328289\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 329207\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 330014\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.900000\n",
            "T: 330786\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.300000\n",
            "T: 331609\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.000000\n",
            "T: 332524\n",
            "T/sec: 58\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 333387\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.700000\n",
            "T: 334127\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.200000\n",
            "T: 334923\n",
            "T/sec: 59\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 2.300000\n",
            "T: 335754\n",
            "T/sec: 59\n",
            "Updating target\n",
            "Summary of last 10 episodes\n",
            "Max reward: 5.000000\n",
            "Min reward: 0.000000\n",
            "Avg reward: 3.600000\n",
            "T: 336515\n",
            "T/sec: 58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qrwVfQdFuTl",
        "colab_type": "code",
        "outputId": "aa9c6925-296e-43df-d85d-58cf97d961d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "agent.scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 10.0,\n",
              " 8.0,\n",
              " 7.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 7.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0,\n",
              " 6.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWYpItUC0WOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if(agent):\n",
        "#   del agent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClYN0fL0wipP",
        "colab_type": "text"
      },
      "source": [
        "texte en gras\n",
        "\n",
        "## Test\n",
        "\n",
        "Run the trained agent (1 episode).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktd8e0qxwipb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.env = gym.wrappers.Monitor(env, \"videos\", force=True)\n",
        "agent.test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjOGcvXrwip_",
        "colab_type": "text"
      },
      "source": [
        "## Render"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw4yFPpswiqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "def ipython_show_video(path: str) -> None:\n",
        "    \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
        "    if not os.path.isfile(path):\n",
        "        raise NameError(\"Cannot access: {}\".format(path))\n",
        "\n",
        "    video = io.open(path, \"r+b\").read()\n",
        "    encoded = base64.b64encode(video)\n",
        "\n",
        "    display(HTML(\n",
        "        data=\"\"\"\n",
        "        <video alt=\"test\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        </video>\n",
        "        \"\"\".format(encoded.decode(\"ascii\"))\n",
        "    ))\n",
        "\n",
        "list_of_files = glob.glob(\"videos/*.mp4\")\n",
        "latest_file = max(list_of_files, key=os.path.getctime)\n",
        "print(latest_file)\n",
        "ipython_show_video(latest_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}