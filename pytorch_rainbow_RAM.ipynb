{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythorch Rainbow\n",
    "This notebook contains basically the code in https://github.com/belepi93/pytorch-rainbow but it has trained on the Qbert game instead of Pong. It has not iterated for 1.4M ooperations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6753,
     "status": "ok",
     "timestamp": 1583432434529,
     "user": {
      "displayName": "Leandro Nascimento Gonçalves de Araujo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj6JecsvnSXbHdOKZlO53EoAiyksBBrzQ_BOWFLlA=s64",
      "userId": "08332749769171265945"
     },
     "user_tz": -60
    },
    "id": "IDZQY5Aa2cSH",
    "outputId": "af8985b0-13fe-493f-d45b-d5264cf71f12"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AGljD1qTh7kX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import time, os\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "417y7gxHj7Sq"
   },
   "source": [
    "#### arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DX1KaSrtj6pS"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "def get_args(arg_s):\n",
    "    parser = argparse.ArgumentParser(description='DQN',argument_default=argparse.SUPPRESS)\n",
    "\n",
    "    # Basic Arguments\n",
    "    parser.add_argument('--seed', type=int, default=1122,\n",
    "                        help='Random seed')\n",
    "    parser.add_argument('--batch-size', type=int, default=32,\n",
    "                        help='Batch size')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "\n",
    "    # Training Arguments\n",
    "    parser.add_argument('--max-frames', type=int, default=1400000, metavar='STEPS',\n",
    "                        help='Number of frames to train')\n",
    "    parser.add_argument('--buffer-size', type=int, default=100000, metavar='CAPACITY',\n",
    "                        help='Maximum memory buffer size')\n",
    "    parser.add_argument('--update-target', type=int, default=1000, metavar='STEPS',\n",
    "                        help='Interval of target network update')\n",
    "    parser.add_argument('--train-freq', type=int, default=1, metavar='STEPS',\n",
    "                        help='Number of steps between optimization step')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, metavar='γ',\n",
    "                        help='Discount factor')\n",
    "    parser.add_argument('--learning-start', type=int, default=10000, metavar='N',\n",
    "                        help='How many steps of the model to collect transitions for before learning starts')\n",
    "    parser.add_argument('--eps_start', type=float, default=1.0,\n",
    "                        help='Start value of epsilon')\n",
    "    parser.add_argument('--eps_final', type=float, default=0.01,\n",
    "                        help='Final value of epsilon')\n",
    "    parser.add_argument('--eps_decay', type=int, default=30000,\n",
    "                        help='Adjustment parameter for epsilon')\n",
    "\n",
    "    # Algorithm Arguments\n",
    "    parser.add_argument('--double', action='store_true',\n",
    "                        help='Enable Double-Q Learning')\n",
    "    parser.add_argument('--dueling', action='store_true',\n",
    "                        help='Enable Dueling Network')\n",
    "    parser.add_argument('--noisy', action='store_true',\n",
    "                        help='Enable Noisy Network')\n",
    "    parser.add_argument('--prioritized-replay', action='store_true',\n",
    "                        help='enable prioritized experience replay')\n",
    "    parser.add_argument('--c51', action='store_true',\n",
    "                        help='enable categorical dqn')\n",
    "    parser.add_argument('--multi-step', type=int, default=1,\n",
    "                        help='N-Step Learning')\n",
    "    parser.add_argument('--Vmin', type=int, default=-10,\n",
    "                        help='Minimum value of support for c51')\n",
    "    parser.add_argument('--Vmax', type=int, default=10,\n",
    "                        help='Maximum value of support for c51')\n",
    "    parser.add_argument('--num-atoms', type=int, default=51,\n",
    "                        help='Number of atom for c51')\n",
    "    parser.add_argument('--alpha', type=float, default=0.6,\n",
    "                        help='Alpha value for prioritized replay')\n",
    "    parser.add_argument('--beta-start', type=float, default=0.4,\n",
    "                        help='Start value of beta for prioritized replay')\n",
    "    parser.add_argument('--beta-frames', type=int, default=100000,\n",
    "                        help='End frame of beta schedule for prioritized replay')\n",
    "    parser.add_argument('--sigma-init', type=float, default=0.4,\n",
    "                        help='Sigma initialization value for NoisyNet')\n",
    "\n",
    "    # Environment Arguments\n",
    "    parser.add_argument('--env', type=str, default='PongNoFrameskip-v4',\n",
    "                        help='Environment Name')\n",
    "    parser.add_argument('--episode-life', type=int, default=1,\n",
    "                        help='Whether env has episode life(1) or not(0)')\n",
    "    parser.add_argument('--clip-rewards', type=int, default=1,\n",
    "                        help='Whether env clip rewards(1) or not(0)')\n",
    "    parser.add_argument('--frame-stack', type=int, default=1,\n",
    "                        help='Whether env stacks frame(1) or not(0)')\n",
    "    parser.add_argument('--scale', type=int, default=0,\n",
    "                        help='Whether env scales(1) or not(0)')\n",
    "\n",
    "    # Evaluation Arguments\n",
    "    parser.add_argument('--load-model', type=str, default=None,\n",
    "                        help='Pretrained model name to load (state dict)')\n",
    "    parser.add_argument('--save-model', type=str, default='model',\n",
    "                        help='Pretrained model name to save (state dict)')\n",
    "    parser.add_argument('--evaluate', action='store_true',\n",
    "                        help='Evaluate only')\n",
    "    parser.add_argument('--render', action='store_true',\n",
    "                        help='Render evaluation agent')\n",
    "    parser.add_argument('--evaluation_interval', type=int, default=10000,\n",
    "                        help='Frames for evaluation interval')\n",
    "\n",
    "    # Optimization Arguments\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, metavar='η',\n",
    "                        help='Learning rate')\n",
    "\n",
    "    args = parser.parse_args(arg_s)\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YiF-fHtti0fP"
   },
   "source": [
    "#### common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6Pkm0wauizwQ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def update_target(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())\n",
    "\n",
    "def epsilon_scheduler(eps_start, eps_final, eps_decay):\n",
    "    def function(frame_idx):\n",
    "        return eps_final + (eps_start - eps_final) * math.exp(-1. * frame_idx / eps_decay)\n",
    "    return function\n",
    "\n",
    "def beta_scheduler(beta_start, beta_frames):\n",
    "    def function(frame_idx):\n",
    "        return min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)\n",
    "    return function\n",
    "\n",
    "def create_log_dir(args):\n",
    "    log_dir = \"\"\n",
    "    if args.multi_step != 1:\n",
    "        log_dir = log_dir + \"{}-step-\".format(args.multi_step)\n",
    "    if args.c51:\n",
    "        log_dir = log_dir + \"c51-\"\n",
    "    if args.prioritized_replay:\n",
    "        log_dir = log_dir + \"per-\"\n",
    "    if args.dueling:\n",
    "        log_dir = log_dir + \"dueling-\"\n",
    "    if args.double:\n",
    "        log_dir = log_dir + \"double-\"\n",
    "    if args.noisy:\n",
    "        log_dir = log_dir + \"noisy-\"\n",
    "    log_dir = log_dir + \"dqn-\"\n",
    "    \n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    log_dir = log_dir + now\n",
    "\n",
    "    log_dir = os.path.join(\"runs\", log_dir)\n",
    "    return log_dir\n",
    "\n",
    "def print_log(frame, prev_frame, prev_time, reward_list, length_list, loss_list):\n",
    "    fps = (frame - prev_frame) / (time.time() - prev_time)\n",
    "    avg_reward = np.mean(reward_list)\n",
    "    avg_length = np.mean(length_list)\n",
    "    avg_loss = np.mean(loss_list) if len(loss_list) != 0 else 0.\n",
    "\n",
    "    print(\"Frame: {:<8} FPS: {:.2f} Avg. Reward: {:.2f} Avg. Length: {:.2f} Avg. Loss: {:.2f}\".format(\n",
    "        frame, fps, avg_reward, avg_length, avg_loss\n",
    "    ))\n",
    "\n",
    "def print_args(args):\n",
    "    print(' ' * 26 + 'Options')\n",
    "    for k, v in vars(args).items():\n",
    "        print(' ' * 26 + k + ': ' + str(v))\n",
    "\n",
    "def save_model(model, args):\n",
    "    fname = \"\"\n",
    "    if args.multi_step != 1:\n",
    "        fname += \"{}-step-\".format(args.multi_step)\n",
    "    if args.c51:\n",
    "        fname += \"c51-\"\n",
    "    if args.prioritized_replay:\n",
    "        fname += \"per-\"\n",
    "    if args.dueling:\n",
    "        fname += \"dueling-\"\n",
    "    if args.double:\n",
    "        fname += \"double-\"\n",
    "    if args.noisy:\n",
    "        fname += \"noisy-\"\n",
    "    fname += \"dqn-{}.pth\".format(args.save_model)\n",
    "    fname = os.path.join(\"models\", fname)\n",
    "\n",
    "    pathlib.Path('models').mkdir(exist_ok=True)\n",
    "    torch.save(model.state_dict(), fname)\n",
    "\n",
    "def load_model(model, args):\n",
    "    if args.load_model is not None:\n",
    "        fname = os.path.join(\"models\", args.load_model)\n",
    "    else:\n",
    "        fname = \"\"\n",
    "        if args.multi_step != 1:\n",
    "            fname += \"{}-step-\".format(args.multi_step)\n",
    "        if args.c51:\n",
    "            fname += \"c51-\"\n",
    "        if args.prioritized_replay:\n",
    "            fname += \"per-\"\n",
    "        if args.dueling:\n",
    "            fname += \"dueling-\"\n",
    "        if args.double:\n",
    "            fname += \"double-\"\n",
    "        if args.noisy:\n",
    "            fname += \"noisy-\"\n",
    "        fname += \"dqn-{}.pth\".format(args.save_model)\n",
    "        fname = os.path.join(\"models\", fname)\n",
    "\n",
    "    if args.device == torch.device(\"cpu\"):\n",
    "        map_location = lambda storage, loc: storage\n",
    "    else:\n",
    "        map_location = None\n",
    "    \n",
    "    if not os.path.exists(fname):\n",
    "        raise ValueError(\"No model saved with name {}\".format(fname))\n",
    "\n",
    "    model.load_state_dict(torch.load(fname, map_location))\n",
    "\n",
    "def set_global_seeds(seed):\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-b7RSNLyjJaC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import operator\n",
    "\n",
    "\n",
    "class SegmentTree(object):\n",
    "    def __init__(self, capacity, operation, neutral_element):\n",
    "        \"\"\"Build a Segment Tree data structure.\n",
    "        https://en.wikipedia.org/wiki/Segment_tree\n",
    "        Can be used as regular array, but with two\n",
    "        important differences:\n",
    "            a) setting item's value is slightly slower.\n",
    "               It is O(lg capacity) instead of O(1).\n",
    "            b) user has access to an efficient `reduce`\n",
    "               operation which reduces `operation` over\n",
    "               a contiguous subsequence of items in the\n",
    "               array.\n",
    "        Paramters\n",
    "        ---------\n",
    "        capacity: int\n",
    "            Total size of the array - must be a power of two.\n",
    "        operation: lambda obj, obj -> obj\n",
    "            and operation for combining elements (eg. sum, max)\n",
    "            must for a mathematical group together with the set of\n",
    "            possible values for array elements.\n",
    "        neutral_element: obj\n",
    "            neutral element for the operation above. eg. float('-inf')\n",
    "            for max and 0 for sum.\n",
    "        \"\"\"\n",
    "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n",
    "        self._capacity = capacity\n",
    "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
    "        self._operation = operation\n",
    "\n",
    "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
    "        if start == node_start and end == node_end:\n",
    "            return self._value[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self._operation(\n",
    "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
    "                )\n",
    "\n",
    "    def reduce(self, start=0, end=None):\n",
    "        \"\"\"Returns result of applying `self.operation`\n",
    "        to a contiguous subsequence of the array.\n",
    "            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n",
    "        Parameters\n",
    "        ----------\n",
    "        start: int\n",
    "            beginning of the subsequence\n",
    "        end: int\n",
    "            end of the subsequences\n",
    "        Returns\n",
    "        -------\n",
    "        reduced: obj\n",
    "            result of reducing self.operation over the specified range of array elements.\n",
    "        \"\"\"\n",
    "        if end is None:\n",
    "            end = self._capacity\n",
    "        if end < 0:\n",
    "            end += self._capacity\n",
    "        end -= 1\n",
    "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        # index of the leaf\n",
    "        idx += self._capacity\n",
    "        self._value[idx] = val\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self._value[idx] = self._operation(\n",
    "                self._value[2 * idx],\n",
    "                self._value[2 * idx + 1]\n",
    "            )\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self._capacity\n",
    "        return self._value[self._capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=operator.add,\n",
    "            neutral_element=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start=0, end=None):\n",
    "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
    "        return super(SumSegmentTree, self).reduce(start, end)\n",
    "\n",
    "    def find_prefixsum_idx(self, prefixsum):\n",
    "        \"\"\"Find the highest index `i` in the array such that\n",
    "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
    "        if array values are probabilities, this function\n",
    "        allows to sample indexes according to the discrete\n",
    "        probability efficiently.\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfixsum: float\n",
    "            upperbound on the sum of array prefix\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            highest index satisfying the prefixsum constraint\n",
    "        \"\"\"\n",
    "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
    "        idx = 1\n",
    "        while idx < self._capacity:  # while non-leaf\n",
    "            if self._value[2 * idx] > prefixsum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefixsum -= self._value[2 * idx]\n",
    "                idx = 2 * idx + 1\n",
    "        return idx - self._capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=min,\n",
    "            neutral_element=float('inf')\n",
    "        )\n",
    "\n",
    "    def min(self, start=0, end=None):\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
    "\n",
    "        return super(MinSegmentTree, self).reduce(start, end)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, size, alpha):\n",
    "        \"\"\"Create Prioritized Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        alpha: float\n",
    "            how much prioritization is used\n",
    "            (0 - no prioritization, 1 - full prioritization)\n",
    "        See Also\n",
    "        --------\n",
    "        ReplayBuffer.__init__\n",
    "        \"\"\"\n",
    "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
    "        assert alpha > 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def push(self, *args, **kwargs):\n",
    "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
    "        idx = self._next_idx\n",
    "        super(PrioritizedReplayBuffer, self).push(*args, **kwargs)\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        for _ in range(batch_size):\n",
    "            # TODO(szymon): should we ensure no repeats?\n",
    "            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        compared to ReplayBuffer.sample\n",
    "        it also returns importance weights and idxes\n",
    "        of sampled experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - no corrections, 1 - full correction)\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        weights: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "            denoting importance weight of each sampled transition\n",
    "        idxes: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "            idexes in buffer of sampled experiences\n",
    "        \"\"\"\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = np.array(weights)\n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return tuple(list(encoded_sample) + [weights, idxes])\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\n",
    "        sets priority of transition at index idxes[i] in buffer\n",
    "        to priorities[i].\n",
    "        Parameters\n",
    "        ----------\n",
    "        idxes: [int]\n",
    "            List of idxes of sampled transitions\n",
    "        priorities: [float]\n",
    "            List of updated priorities corresponding to\n",
    "            transitions at the sampled idxes denoted by\n",
    "            variable `idxes`.\n",
    "        \"\"\"\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = priority ** self._alpha\n",
    "            self._it_min[idx] = priority ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzJep53AlVx_"
   },
   "source": [
    "#### storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TeChimZvlXYT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class NaivePrioritizedBuffer(object):\n",
    "    def __init__(self, capacity, prob_alpha=0.6):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []\n",
    "        self.pos        = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        \n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "        \n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        \n",
    "        probs  = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total    = len(self.buffer)\n",
    "        weights  = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        batch       = list(zip(*samples))\n",
    "        states      = np.concatenate(batch[0])\n",
    "        actions     = batch[1]\n",
    "        rewards     = batch[2]\n",
    "        next_states = np.concatenate(batch[3])\n",
    "        dones       = batch[4]\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, weights, indices\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kzx5HEKrlLfN"
   },
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yNtUV7S6lNGA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def DQN(env, args):\n",
    "    if args.c51:\n",
    "        if args.dueling:\n",
    "            model = CategoricalDuelingDQN(env, args.noisy, args.sigma_init,\n",
    "                                          args.Vmin, args.Vmax, args.num_atoms, args.batch_size)\n",
    "        else:\n",
    "            model = CategoricalDQN(env, args.noisy, args.sigma_init,\n",
    "                                   args.Vmin, args.Vmax, args.num_atoms, args.batch_size)\n",
    "    else:\n",
    "        if args.dueling:\n",
    "            model = DuelingDQN(env, args.noisy, args.sigma_init)\n",
    "        else:\n",
    "            model = DQNBase(env, args.noisy, args.sigma_init)\n",
    "            \n",
    "    return model\n",
    "\n",
    "\n",
    "class DQNBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic DQN + NoisyNet\n",
    "    Noisy Networks for Exploration\n",
    "    https://arxiv.org/abs/1706.10295\n",
    "    \n",
    "    parameters\n",
    "    ---------\n",
    "    env         environment(openai gym)\n",
    "    noisy       boolean value for NoisyNet. \n",
    "                If this is set to True, self.Linear will be NoisyLinear module\n",
    "    \"\"\"\n",
    "    def __init__(self, env, noisy, sigma_init):\n",
    "        super(DQNBase, self).__init__()\n",
    "        \n",
    "        self.input_shape = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.noisy = noisy\n",
    "\n",
    "        if noisy:\n",
    "            self.Linear = partial(NoisyLinear, sigma_init=sigma_init)\n",
    "        else:\n",
    "            self.Linear = nn.Linear\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            self.Linear(self._feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            self.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def _feature_size(self):\n",
    "        return self.features(torch.zeros(1, *self.input_shape)).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        state       torch.Tensor with appropritate device type\n",
    "        epsilon     epsilon for epsilon-greedy\n",
    "        \"\"\"\n",
    "        if random.random() > epsilon or self.noisy:  # NoisyNet does not use e-greedy\n",
    "            with torch.no_grad():\n",
    "                state   = state.unsqueeze(0)\n",
    "                q_value = self.forward(state)\n",
    "                action  = q_value.max(1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        return action\n",
    "        \n",
    "    def update_noisy_modules(self):\n",
    "        if self.noisy:\n",
    "            self.noisy_modules = [module for module in self.modules() if isinstance(module, NoisyLinear)]\n",
    "    \n",
    "    def sample_noise(self):\n",
    "        for module in self.noisy_modules:\n",
    "            module.sample_noise()\n",
    "\n",
    "    def remove_noise(self):\n",
    "        for module in self.noisy_modules:\n",
    "            module.remove_noise()\n",
    "\n",
    "\n",
    "class DuelingDQN(DQNBase):\n",
    "    \"\"\"\n",
    "    Dueling Network Architectures for Deep Reinforcement Learning\n",
    "    https://arxiv.org/abs/1511.06581\n",
    "    \"\"\"\n",
    "    def __init__(self, env, noisy, sigma_init):\n",
    "        super(DuelingDQN, self).__init__(env, noisy, sigma_init)\n",
    "        \n",
    "        self.advantage = self.fc\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            self.Linear(self._feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            self.Linear(512, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        return value + advantage - advantage.mean(1, keepdim=True)\n",
    "\n",
    "\n",
    "class CategoricalDQN(DQNBase):\n",
    "    \"\"\"\n",
    "    A Distributional Perspective on Reinforcement Learning\n",
    "    https://arxiv.org/abs/1707.06887\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, noisy, sigma_init, Vmin, Vmax, num_atoms, batch_size):\n",
    "        super(CategoricalDQN, self).__init__(env, noisy, sigma_init)\n",
    "    \n",
    "        support = torch.linspace(Vmin, Vmax, num_atoms)\n",
    "        offset = torch.linspace(0, (batch_size - 1) * num_atoms, batch_size).long()\\\n",
    "            .unsqueeze(1).expand(batch_size, num_atoms)\n",
    "\n",
    "        self.register_buffer('support', support)\n",
    "        self.register_buffer('offset', offset)\n",
    "        self.num_atoms = num_atoms\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            self.Linear(self._feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            self.Linear(512, self.num_actions * self.num_atoms),\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x.view(-1, self.num_atoms))\n",
    "        x = x.view(-1, self.num_actions, self.num_atoms)\n",
    "        return x\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        state       torch.Tensor with appropritate device type\n",
    "        epsilon     epsilon for epsilon-greedy\n",
    "        \"\"\"\n",
    "        if random.random() > epsilon or self.noisy:  # NoisyNet does not use e-greedy\n",
    "            with torch.no_grad():\n",
    "                state   = state.unsqueeze(0)\n",
    "                q_dist = self.forward(state)\n",
    "                q_value = (q_dist * self.support).sum(2)\n",
    "                action  = q_value.max(1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        return action\n",
    "\n",
    "\n",
    "class CategoricalDuelingDQN(CategoricalDQN):\n",
    "\n",
    "    def __init__(self, env, noisy, sigma_init, Vmin, Vmax, num_atoms, batch_size):\n",
    "        super(CategoricalDuelingDQN, self).__init__(env, noisy, sigma_init, Vmin, Vmax, num_atoms, batch_size)\n",
    "        \n",
    "        self.advantage = self.fc\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            self.Linear(self._feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            self.Linear(512, num_atoms)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        advantage = self.advantage(x).view(-1, self.num_actions, self.num_atoms)\n",
    "        value = self.value(x).view(-1, 1, self.num_atoms)\n",
    "\n",
    "        x = value + advantage - advantage.mean(1, keepdim=True)\n",
    "        x = self.softmax(x.view(-1, self.num_atoms))\n",
    "        x = x.view(-1, self.num_actions, self.num_atoms)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, sigma_init):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features \n",
    "        self.sigma_init = sigma_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
    "\n",
    "        self.register_buffer('sample_weight_in', torch.FloatTensor(in_features))\n",
    "        self.register_buffer('sample_weight_out', torch.FloatTensor(out_features))\n",
    "        self.register_buffer('sample_bias_out', torch.FloatTensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.sample_noise()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma.mul(self.weight_epsilon)\n",
    "            bias = self.bias_mu + self.bias_sigma.mul(self.bias_epsilon)\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        \n",
    "        return F.linear(x, weight, bias)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.weight_mu.size(1))\n",
    "\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma_init / math.sqrt(self.weight_sigma.size(1)))\n",
    "\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.sigma_init / math.sqrt(self.bias_sigma.size(0)))\n",
    "\n",
    "    def sample_noise(self):\n",
    "        self.sample_weight_in = self._scale_noise(self.sample_weight_in)\n",
    "        self.sample_weight_out = self._scale_noise(self.sample_weight_out)\n",
    "        self.sample_bias_out = self._scale_noise(self.sample_bias_out)\n",
    "\n",
    "        self.weight_epsilon.copy_(self.sample_weight_out.ger(self.sample_weight_in))\n",
    "        self.bias_epsilon.copy_(self.sample_bias_out)\n",
    "    \n",
    "    def _scale_noise(self, x):\n",
    "        x = x.normal_()\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "f-_3kGjeje6Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not believe how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=2)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n",
    "\n",
    "def make_atari(env_id):\n",
    "    env = gym.make(env_id)\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    return env\n",
    "\n",
    "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "    \"\"\"\n",
    "    if episode_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "    if frame_stack:\n",
    "        env = FrameStack(env, 4)\n",
    "    return env\n",
    "\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Image shape to num_channels x weight x height\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.swapaxes(observation, 2, 0)\n",
    "    \n",
    "\n",
    "def wrap_pytorch(env):\n",
    "    return ImageToPyTorch(env)\n",
    "\n",
    "def wrap_atari_dqn(env, args):\n",
    "    env = wrap_deepmind(env, \n",
    "                        episode_life=args.episode_life,\n",
    "                        clip_rewards=args.clip_rewards,\n",
    "                        frame_stack=args.frame_stack,\n",
    "                        scale=args.scale)\n",
    "    env = wrap_pytorch(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fe5Xmguyi4eL"
   },
   "source": [
    "#### train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5Db_-JsQiAME"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(env, args): \n",
    "    current_model = DQN(env, args).to(args.device)\n",
    "    current_model.eval()\n",
    "\n",
    "    load_model(current_model, args)\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        if args.render:\n",
    "            env.render()\n",
    "\n",
    "        action = current_model.act(torch.FloatTensor(state).to(args.device), 0.)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(\"Test Result - Reward {} Length {}\".format(episode_reward, episode_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JOe2YQ-OiOPE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(env, args, writer): \n",
    "    current_model = DQN(env, args).to(args.device)\n",
    "    target_model = DQN(env, args).to(args.device)\n",
    "\n",
    "    if args.noisy:\n",
    "        current_model.update_noisy_modules()\n",
    "        target_model.update_noisy_modules()\n",
    "\n",
    "    if args.load_model and os.path.isfile(args.load_model):\n",
    "        load_model(current_model, args)\n",
    "\n",
    "    epsilon_by_frame = epsilon_scheduler(args.eps_start, args.eps_final, args.eps_decay)\n",
    "    beta_by_frame = beta_scheduler(args.beta_start, args.beta_frames)\n",
    "\n",
    "    if args.prioritized_replay:\n",
    "        replay_buffer = PrioritizedReplayBuffer(args.buffer_size, args.alpha)\n",
    "    else:\n",
    "        replay_buffer = ReplayBuffer(args.buffer_size)\n",
    "    \n",
    "    state_deque = deque(maxlen=args.multi_step)\n",
    "    reward_deque = deque(maxlen=args.multi_step)\n",
    "    action_deque = deque(maxlen=args.multi_step)\n",
    "\n",
    "    optimizer = optim.Adam(current_model.parameters(), lr=args.lr)\n",
    "\n",
    "    reward_list, length_list, loss_list = [], [], []\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "\n",
    "    prev_time = time.time()\n",
    "    prev_frame = 1\n",
    "\n",
    "    state = env.reset()\n",
    "    for frame_idx in range(1, args.max_frames + 1):\n",
    "        if args.render:\n",
    "            env.render()\n",
    "\n",
    "        if args.noisy:\n",
    "            current_model.sample_noise()\n",
    "            target_model.sample_noise()\n",
    "\n",
    "        epsilon = epsilon_by_frame(frame_idx)\n",
    "        action = current_model.act(torch.FloatTensor(state).to(args.device), epsilon)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state_deque.append(state)\n",
    "        reward_deque.append(reward)\n",
    "        action_deque.append(action)\n",
    "\n",
    "        if len(state_deque) == args.multi_step or done:\n",
    "            n_reward = multi_step_reward(reward_deque, args.gamma)\n",
    "            n_state = state_deque[0]\n",
    "            n_action = action_deque[0]\n",
    "            replay_buffer.push(n_state, n_action, n_reward, next_state, np.float32(done))\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            reward_list.append(episode_reward)\n",
    "            length_list.append(episode_length)\n",
    "            writer.add_scalar(\"data/episode_reward\", episode_reward, frame_idx)\n",
    "            writer.add_scalar(\"data/episode_length\", episode_length, frame_idx)\n",
    "            episode_reward, episode_length = 0, 0\n",
    "            state_deque.clear()\n",
    "            reward_deque.clear()\n",
    "            action_deque.clear()\n",
    "\n",
    "        if len(replay_buffer) > args.learning_start and frame_idx % args.train_freq == 0:\n",
    "            beta = beta_by_frame(frame_idx)\n",
    "            loss = compute_td_loss(current_model, target_model, replay_buffer, optimizer, args, beta)\n",
    "            loss_list.append(loss.item())\n",
    "            writer.add_scalar(\"data/loss\", loss.item(), frame_idx)\n",
    "\n",
    "        if frame_idx % args.update_target == 0:\n",
    "            update_target(current_model, target_model)\n",
    "\n",
    "        if frame_idx % args.evaluation_interval == 0:\n",
    "            print_log(frame_idx, prev_frame, prev_time, reward_list, length_list, loss_list)\n",
    "            reward_list.clear(), length_list.clear(), loss_list.clear()\n",
    "            prev_frame = frame_idx\n",
    "            prev_time = time.time()\n",
    "            save_model(current_model, args)\n",
    "\n",
    "    save_model(current_model, args)\n",
    "\n",
    "\n",
    "def compute_td_loss(current_model, target_model, replay_buffer, optimizer, args, beta=None):\n",
    "    \"\"\"\n",
    "    Calculate loss and optimize for non-c51 algorithm\n",
    "    \"\"\"\n",
    "    if args.prioritized_replay:\n",
    "        state, action, reward, next_state, done, weights, indices = replay_buffer.sample(args.batch_size, beta)\n",
    "    else:\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(args.batch_size)\n",
    "        weights = torch.ones(args.batch_size)\n",
    "\n",
    "    state = torch.FloatTensor(np.float32(state)).to(args.device)\n",
    "    next_state = torch.FloatTensor(np.float32(next_state)).to(args.device)\n",
    "    action = torch.LongTensor(action).to(args.device)\n",
    "    reward = torch.FloatTensor(reward).to(args.device)\n",
    "    done = torch.FloatTensor(done).to(args.device)\n",
    "    weights = torch.FloatTensor(weights).to(args.device)\n",
    "\n",
    "    if not args.c51:\n",
    "        q_values = current_model(state)\n",
    "        target_next_q_values = target_model(next_state)\n",
    "\n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        if args.double:\n",
    "            next_q_values = current_model(next_state)\n",
    "            next_actions = next_q_values.max(1)[1].unsqueeze(1)\n",
    "            next_q_value = target_next_q_values.gather(1, next_actions).squeeze(1)\n",
    "        else:\n",
    "            next_q_value = target_next_q_values.max(1)[0]\n",
    "\n",
    "        expected_q_value = reward + (args.gamma ** args.multi_step) * next_q_value * (1 - done)\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_value, expected_q_value.detach(), reduction='none')\n",
    "        if args.prioritized_replay:\n",
    "            prios = torch.abs(loss) + 1e-5\n",
    "        loss = (loss * weights).mean()\n",
    "    \n",
    "    else:\n",
    "        q_dist = current_model(state)\n",
    "        action = action.unsqueeze(1).unsqueeze(1).expand(args.batch_size, 1, args.num_atoms)\n",
    "        q_dist = q_dist.gather(1, action).squeeze(1)\n",
    "        q_dist.data.clamp_(0.01, 0.99)\n",
    "\n",
    "        target_dist = projection_distribution(current_model, target_model, next_state, reward, done, \n",
    "                                              target_model.support, target_model.offset, args)\n",
    "\n",
    "        loss = - (target_dist * q_dist.log()).sum(1)\n",
    "        if args.prioritized_replay:\n",
    "            prios = torch.abs(loss) + 1e-6\n",
    "        loss = (loss * weights).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if args.prioritized_replay:\n",
    "        replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def projection_distribution(current_model, target_model, next_state, reward, done, support, offset, args):\n",
    "    delta_z = float(args.Vmax - args.Vmin) / (args.num_atoms - 1)\n",
    "\n",
    "    target_next_q_dist = target_model(next_state)\n",
    "\n",
    "    if args.double:\n",
    "        next_q_dist = current_model(next_state)\n",
    "        next_action = (next_q_dist * support).sum(2).max(1)[1]\n",
    "    else:\n",
    "        next_action = (target_next_q_dist * support).sum(2).max(1)[1]\n",
    "\n",
    "    next_action = next_action.unsqueeze(1).unsqueeze(1).expand(target_next_q_dist.size(0), 1, target_next_q_dist.size(2))\n",
    "    target_next_q_dist = target_next_q_dist.gather(1, next_action).squeeze(1)\n",
    "\n",
    "    reward = reward.unsqueeze(1).expand_as(target_next_q_dist)\n",
    "    done = done.unsqueeze(1).expand_as(target_next_q_dist)\n",
    "    support = support.unsqueeze(0).expand_as(target_next_q_dist)\n",
    "\n",
    "    Tz = reward + args.gamma * support * (1 - done)\n",
    "    Tz = Tz.clamp(min=args.Vmin, max=args.Vmax)\n",
    "    b = (Tz - args.Vmin) / delta_z\n",
    "    l = b.floor().long()\n",
    "    u = b.ceil().long()\n",
    "\n",
    "    target_dist = target_next_q_dist.clone().zero_()\n",
    "    target_dist.view(-1).index_add_(0, (l + offset).view(-1), (target_next_q_dist * (u.float() - b)).view(-1))\n",
    "    target_dist.view(-1).index_add_(0, (u + offset).view(-1), (target_next_q_dist * (b - l.float())).view(-1))\n",
    "\n",
    "    return target_dist\n",
    "\n",
    "def multi_step_reward(rewards, gamma):\n",
    "    ret = 0.\n",
    "    for idx, reward in enumerate(rewards):\n",
    "        ret += reward * (gamma ** idx)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzyLDxnMxJ7l"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "eams7MMoiAHN",
    "outputId": "9d099ec2-ab41-454b-ab16-53192dabbf99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Options\n",
      "                          seed: 1122\n",
      "                          batch_size: 32\n",
      "                          no_cuda: False\n",
      "                          max_frames: 1400000\n",
      "                          buffer_size: 100000\n",
      "                          update_target: 1000\n",
      "                          train_freq: 1\n",
      "                          gamma: 0.99\n",
      "                          learning_start: 10000\n",
      "                          eps_start: 1.0\n",
      "                          eps_final: 0.01\n",
      "                          eps_decay: 30000\n",
      "                          multi_step: 3\n",
      "                          Vmin: -10\n",
      "                          Vmax: 10\n",
      "                          num_atoms: 51\n",
      "                          alpha: 0.6\n",
      "                          beta_start: 0.4\n",
      "                          beta_frames: 100000\n",
      "                          sigma_init: 0.4\n",
      "                          env: QbertNoFrameskip-v4\n",
      "                          episode_life: 1\n",
      "                          clip_rewards: 1\n",
      "                          frame_stack: 1\n",
      "                          scale: 0\n",
      "                          load_model: None\n",
      "                          save_model: model\n",
      "                          evaluation_interval: 10000\n",
      "                          lr: 0.0001\n",
      "                          double: True\n",
      "                          dueling: True\n",
      "                          noisy: True\n",
      "                          c51: True\n",
      "                          prioritized_replay: True\n",
      "                          cuda: True\n",
      "                          device: cuda\n",
      "                          render: False\n",
      "Frame: 20000    FPS: 21.84 Avg. Reward: 1.34 Avg. Length: 76.88 Avg. Loss: 0.04\n",
      "Frame: 30000    FPS: 21.36 Avg. Reward: 1.99 Avg. Length: 84.73 Avg. Loss: 0.02\n",
      "Frame: 40000    FPS: 21.38 Avg. Reward: 2.73 Avg. Length: 89.10 Avg. Loss: 0.01\n",
      "Frame: 50000    FPS: 21.37 Avg. Reward: 2.40 Avg. Length: 89.24 Avg. Loss: 0.01\n",
      "Frame: 60000    FPS: 21.55 Avg. Reward: 2.70 Avg. Length: 96.64 Avg. Loss: 0.00\n",
      "Frame: 70000    FPS: 21.97 Avg. Reward: 2.83 Avg. Length: 98.07 Avg. Loss: 0.00\n",
      "Frame: 80000    FPS: 22.01 Avg. Reward: 2.84 Avg. Length: 90.83 Avg. Loss: 0.00\n",
      "Frame: 90000    FPS: 22.00 Avg. Reward: 3.46 Avg. Length: 103.75 Avg. Loss: 0.00\n",
      "Frame: 100000   FPS: 22.01 Avg. Reward: 3.96 Avg. Length: 107.39 Avg. Loss: 0.00\n",
      "Frame: 110000   FPS: 22.01 Avg. Reward: 4.05 Avg. Length: 102.93 Avg. Loss: 0.00\n",
      "Frame: 120000   FPS: 22.00 Avg. Reward: 4.09 Avg. Length: 106.56 Avg. Loss: 0.00\n",
      "Frame: 130000   FPS: 22.00 Avg. Reward: 4.33 Avg. Length: 103.99 Avg. Loss: 0.00\n",
      "Frame: 140000   FPS: 22.02 Avg. Reward: 4.43 Avg. Length: 114.08 Avg. Loss: 0.00\n",
      "Frame: 150000   FPS: 22.36 Avg. Reward: 4.77 Avg. Length: 112.32 Avg. Loss: 0.00\n",
      "Frame: 160000   FPS: 22.42 Avg. Reward: 4.57 Avg. Length: 105.56 Avg. Loss: 0.00\n",
      "Frame: 170000   FPS: 22.38 Avg. Reward: 5.05 Avg. Length: 107.56 Avg. Loss: 0.00\n",
      "Frame: 180000   FPS: 22.41 Avg. Reward: 4.84 Avg. Length: 104.78 Avg. Loss: 0.00\n",
      "Frame: 190000   FPS: 22.43 Avg. Reward: 4.77 Avg. Length: 104.70 Avg. Loss: 0.00\n",
      "Frame: 200000   FPS: 22.45 Avg. Reward: 4.97 Avg. Length: 102.95 Avg. Loss: 0.00\n",
      "Frame: 210000   FPS: 22.37 Avg. Reward: 4.75 Avg. Length: 99.00 Avg. Loss: 0.00\n",
      "Frame: 220000   FPS: 22.66 Avg. Reward: 4.73 Avg. Length: 103.01 Avg. Loss: 0.00\n",
      "Frame: 230000   FPS: 22.78 Avg. Reward: 4.27 Avg. Length: 93.82 Avg. Loss: 0.00\n",
      "Frame: 240000   FPS: 22.79 Avg. Reward: 4.62 Avg. Length: 99.58 Avg. Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "arg_s =  [\"--multi-step\", \"3\", \"--double\", \"--dueling\", \"--noisy\", \"--c51\", \n",
    "          \"--prioritized-replay\",'--env','QbertNoFrameskip-v4']\n",
    "args = get_args(arg_s)\n",
    "args.render = False\n",
    "print_args(args)\n",
    "\n",
    "log_dir = create_log_dir(args)\n",
    "#if not args.evaluate:\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "env = make_atari(args.env)\n",
    "env = wrap_atari_dqn(env, args)\n",
    "\n",
    "set_global_seeds(args.seed)\n",
    "env.seed(args.seed)\n",
    "\n",
    "#if args.evaluate:\n",
    "\n",
    "train(env, args, writer)\n",
    "\n",
    "writer.export_scalars_to_json(os.path.join(log_dir, \"all_scalars.json\"))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pTLEM6W-5VzH"
   },
   "outputs": [],
   "source": [
    "test(env, args)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMXTWTar9uhhKb9a7ksG6D1",
   "name": "artari_with_RAM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
